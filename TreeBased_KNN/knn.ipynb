{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49c39c00-168d-4286-bbb8-a5c563a6c684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train: (5000, 20), test: (500, 19)\n",
      "Combined df shape: (5007, 20), target length: 4507\n",
      "Feature engineering complete.\n",
      "Prepared raw X: (4507, 33), raw X_test: (500, 33), y: (4507,)\n",
      "Using feature groups sizes -> numeric: 7 skewed: 7 categorical: 3 binary: 3\n",
      "Train/validation split done. Train: (3605, 33) Val: (902, 33)\n",
      "\n",
      "Starting GridSearchCV (this may take a while)...\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "Grid search done.\n",
      "Best params: {'knn__n_neighbors': 10, 'knn__p': 1, 'knn__weights': 'distance', 'pca__n_components': 20}\n",
      "Best CV RMSE (log-scale): 0.7951077855594888\n",
      "\n",
      "KNN + PCA Validation Results (on actual cost scale):\n",
      "Validation RMSE: $130,328.69\n",
      "Validation R-squared: 0.1777\n",
      "Validation MAE: $13,341.20\n",
      "\n",
      "Saved submission_knn_pca.csv\n",
      "            Hospital_Id  Transport_Cost\n",
      "0          fffe33003400      231.466655\n",
      "1  fffe3700330036003600      239.829060\n",
      "2  fffe3300390038003400     1430.538048\n",
      "3      fffe310030003900      337.033489\n",
      "4  fffe3700330031003200     1356.466936\n",
      "\n",
      "PCA cumulative explained variance (%): [20.90235677 37.1733472  45.9955837  53.96951423 59.94808577 65.86458093\n",
      " 71.57935109 76.42056998 79.47643559 82.07802041]\n"
     ]
    }
   ],
   "source": [
    "#PCA + KNN (GridSearchCV)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Load Data\n",
    "try:\n",
    "    train_df_raw = pd.read_csv(\"medical_cost_prediction/train.csv\")\n",
    "    test_df_raw = pd.read_csv(\"medical_cost_prediction/test.csv\")\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        train_df_raw = pd.read_csv(\"trainvad.csv\")\n",
    "        test_df_raw = pd.read_csv(\"test.csv\")\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\"Could not find train/test CSV files. Put them in expected paths.\") from e\n",
    "\n",
    "print(f\"Loaded train: {train_df_raw.shape}, test: {test_df_raw.shape}\")\n",
    "\n",
    "#Prepare target (log transform)\n",
    "train_df_raw = train_df_raw[train_df_raw[\"Transport_Cost\"] > 0].copy()\n",
    "target_log = np.log1p(train_df_raw[\"Transport_Cost\"]).reset_index(drop=True)\n",
    "train_df_raw = train_df_raw.drop(columns=[\"Transport_Cost\"])\n",
    "\n",
    "#mark source and combine for uniform feature engineering\n",
    "train_df_raw[\"source\"] = \"train\"\n",
    "test_df_raw[\"source\"] = \"test\"\n",
    "df = pd.concat([train_df_raw, test_df_raw], ignore_index=True)\n",
    "print(f\"Combined df shape: {df.shape}, target length: {len(target_log)}\")\n",
    "\n",
    "#Feature engineering function(keeps your logic)\n",
    "missing_cols = [\n",
    "    'Supplier_Reliability', 'Equipment_Height', 'Equipment_Width',\n",
    "    'Equipment_Weight', 'Equipment_Type', 'Transport_Method', 'Rural_Hospital'\n",
    "]\n",
    "\n",
    "def preprocess_features(df_in):\n",
    "    df_out = df_in.copy()\n",
    "\n",
    "    if \"Supplier_Name\" in df_out.columns:\n",
    "        df_out = df_out.drop(columns=[\"Supplier_Name\"])\n",
    "\n",
    "    # Missing indicators\n",
    "    for col in missing_cols:\n",
    "        if col in df_out.columns:\n",
    "            df_out[col + \"_Is_Missing\"] = df_out[col].isnull().astype(int)\n",
    "\n",
    "    # Date handling\n",
    "    if \"Order_Placed_Date\" in df_out.columns and \"Delivery_Date\" in df_out.columns:\n",
    "        df_out[\"Order_Placed_Date\"] = pd.to_datetime(df_out[\"Order_Placed_Date\"], errors=\"coerce\")\n",
    "        df_out[\"Delivery_Date\"] = pd.to_datetime(df_out[\"Delivery_Date\"], errors=\"coerce\")\n",
    "        df_out[\"Delivery_Time_Days\"] = (df_out[\"Delivery_Date\"] - df_out[\"Order_Placed_Date\"]).dt.days.clip(lower=0).fillna(0).astype(int)\n",
    "        df_out[\"Order_Year\"] = df_out[\"Order_Placed_Date\"].dt.year.fillna(0).astype(int)\n",
    "        df_out[\"Order_Month\"] = df_out[\"Order_Placed_Date\"].dt.month.fillna(0).astype(int)\n",
    "        df_out[\"Order_DayOfWeek\"] = df_out[\"Order_Placed_Date\"].dt.dayofweek.fillna(0).astype(int)\n",
    "        df_out = df_out.drop(columns=[\"Order_Placed_Date\", \"Delivery_Date\"], errors=\"ignore\")\n",
    "\n",
    "    # Binary mapping\n",
    "    binary_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service',\n",
    "                   'Fragile_Equipment', 'Rural_Hospital', 'Hospital_Info']\n",
    "    for c in binary_cols:\n",
    "        if c in df_out.columns:\n",
    "            df_out[c] = df_out[c].map({\"Yes\": 1, \"No\": 0}).fillna(0).astype(int)\n",
    "\n",
    "    # Fill numeric-ish columns\n",
    "    if \"Equipment_Height\" in df_out.columns:\n",
    "        df_out[\"Equipment_Height\"] = df_out[\"Equipment_Height\"].fillna(1)\n",
    "    if \"Equipment_Width\" in df_out.columns:\n",
    "        df_out[\"Equipment_Width\"] = df_out[\"Equipment_Width\"].fillna(1)\n",
    "    if \"Equipment_Weight\" in df_out.columns:\n",
    "        df_out[\"Equipment_Weight\"] = df_out[\"Equipment_Weight\"].fillna(0)\n",
    "    if \"Equipment_Value\" in df_out.columns:\n",
    "        df_out[\"Equipment_Value\"] = df_out[\"Equipment_Value\"].fillna(0)\n",
    "\n",
    "    # Derived features\n",
    "    if {\"Equipment_Height\", \"Equipment_Width\"}.issubset(df_out.columns):\n",
    "        df_out[\"Equipment_Area\"] = df_out[\"Equipment_Height\"] * df_out[\"Equipment_Width\"]\n",
    "    else:\n",
    "        df_out[\"Equipment_Area\"] = 0\n",
    "\n",
    "    if {\"Equipment_Value\", \"Equipment_Weight\"}.issubset(df_out.columns):\n",
    "        df_out[\"Value_Density\"] = df_out[\"Equipment_Value\"] / (df_out[\"Equipment_Weight\"] + 1e-6)\n",
    "    else:\n",
    "        df_out[\"Value_Density\"] = 0\n",
    "\n",
    "    df_out[\"Value_per_Height\"] = df_out.get(\"Equipment_Value\", 0) / (df_out.get(\"Equipment_Height\", 1) + 1e-6)\n",
    "    df_out[\"Value_per_Width\"] = df_out.get(\"Equipment_Value\", 0) / (df_out.get(\"Equipment_Width\", 1) + 1e-6)\n",
    "    df_out[\"Weight_per_Area\"] = df_out.get(\"Equipment_Weight\", 0) / (df_out.get(\"Equipment_Area\", 1) + 1e-6)\n",
    "    df_out[\"ValuePerArea\"] = df_out.get(\"Equipment_Value\", 0) / (df_out.get(\"Equipment_Area\", 1) + 1e-6)\n",
    "\n",
    "    # OPTIONAL: additional interactions that often help (safe defaults)\n",
    "    # only add if columns exist\n",
    "    if {\"Equipment_Height\", \"Equipment_Width\", \"Equipment_Weight\"}.issubset(df_out.columns):\n",
    "        df_out[\"Volume\"] = df_out[\"Equipment_Height\"] * df_out[\"Equipment_Width\"] * (df_out[\"Equipment_Weight\"] + 1e-6)\n",
    "    else:\n",
    "        df_out[\"Volume\"] = 0\n",
    "\n",
    "    return df_out\n",
    "\n",
    "df_featured = preprocess_features(df)\n",
    "print(\"Feature engineering complete.\")\n",
    "\n",
    "#Build raw X(keep original columns; pipeline will transform)\n",
    "#Drop ID and source from model inputs (we keep Hospital_Id separately for submission)\n",
    "X_all_raw = df_featured.drop(columns=['Hospital_Id', 'source'], errors='ignore')\n",
    "# Split raw train vs raw test by source\n",
    "train_mask = df_featured['source'] == 'train'\n",
    "X_raw = X_all_raw.loc[train_mask.values].reset_index(drop=True)\n",
    "X_test_raw = X_all_raw.loc[~train_mask.values].reset_index(drop=True)\n",
    "y = target_log.reset_index(drop=True)\n",
    "\n",
    "print(f\"Prepared raw X: {X_raw.shape}, raw X_test: {X_test_raw.shape}, y: {y.shape}\")\n",
    "\n",
    "#Identify feature groups\n",
    "numeric_features = ['Supplier_Reliability', \"Value_per_Height\", \"Value_per_Width\",\n",
    "                    \"Weight_per_Area\", 'Delivery_Time_Days', \"ValuePerArea\", \"Volume\"]\n",
    "skewed_features = ['Equipment_Weight', 'Base_Transport_Fee', 'Value_Density',\n",
    "                   'Equipment_Area', \"Equipment_Height\", \"Equipment_Width\", \"Equipment_Value\"]\n",
    "categorical_features = ['Transport_Method', 'Order_DayOfWeek', 'Equipment_Type']\n",
    "binary_features = ['Urgent_Shipping', 'Installation_Service', 'Hospital_Info']\n",
    "\n",
    "def filter_existing(cols, df_cols):\n",
    "    return [c for c in cols if c in df_cols]\n",
    "\n",
    "numeric_features = filter_existing(numeric_features, X_all_raw.columns)\n",
    "skewed_features = filter_existing(skewed_features, X_all_raw.columns)\n",
    "categorical_features = filter_existing(categorical_features, X_all_raw.columns)\n",
    "binary_features = filter_existing(binary_features, X_all_raw.columns)\n",
    "\n",
    "print(\"Using feature groups sizes -> numeric:\", len(numeric_features),\n",
    "      \"skewed:\", len(skewed_features), \"categorical:\", len(categorical_features), \"binary:\", len(binary_features))\n",
    "\n",
    "#Build ColumnTransformer (preprocessing)\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())   # scale numeric before PCA\n",
    "])\n",
    "\n",
    "skewed_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('skew', skewed_pipeline, skewed_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ],\n",
    "    remainder='drop'  # drop any other columns\n",
    ")\n",
    "\n",
    "#Train/Validation split on raw data\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(X_raw, y, test_size=0.2, random_state=42)\n",
    "print(\"Train/validation split done. Train:\", X_train_raw.shape, \"Val:\", X_val_raw.shape)\n",
    "\n",
    "#Full pipeline: preprocessor\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=30, random_state=42)),\n",
    "    ('post_scaler', StandardScaler()),   # scale PCA outputs before KNN\n",
    "    ('knn', KNeighborsRegressor(n_jobs=-1))\n",
    "])\n",
    "\n",
    "#Grid search parameters (tune PCA n_components and KNN params)\n",
    "param_grid = {\n",
    "    'pca__n_components': [20, 30, 40],             \n",
    "    'knn__n_neighbors': [5, 10, 15, 20],            \n",
    "    'knn__p': [1, 2],                              \n",
    "    'knn__weights': ['distance', 'uniform']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV (this may take a while)...\")\n",
    "grid_search.fit(X_train_raw, y_train)\n",
    "print(\"Grid search done.\")\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV RMSE (log-scale):\", -grid_search.best_score_)\n",
    "\n",
    "#Validation evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred_log = best_model.predict(X_val_raw)\n",
    "\n",
    "# back-transform to original cost scale\n",
    "y_val_actual = np.expm1(y_val)\n",
    "y_val_pred_actual = np.maximum(0, np.expm1(y_val_pred_log))\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val_actual, y_val_pred_actual))\n",
    "r2 = r2_score(y_val_actual, y_val_pred_actual)\n",
    "mae = mean_absolute_error(y_val_actual, y_val_pred_actual)\n",
    "\n",
    "print(\"\\nKNN + PCA Validation Results (on actual cost scale):\")\n",
    "print(f\"Validation RMSE: ${rmse:,.2f}\")\n",
    "print(f\"Validation R-squared: {r2:.4f}\")\n",
    "print(f\"Validation MAE: ${mae:,.2f}\")\n",
    "\n",
    "#Predict on test set and save submission\n",
    "\n",
    "# Use best_model\n",
    "test_pred_log = best_model.predict(X_test_raw)\n",
    "test_pred = np.maximum(0, np.expm1(test_pred_log))\n",
    "\n",
    "# Ensure Hospital_Id exists in original test_df_raw\n",
    "if 'Hospital_Id' not in test_df_raw.columns:\n",
    "    # if not present, create an index-based id\n",
    "    submission_ids = np.arange(len(test_df_raw))\n",
    "else:\n",
    "    submission_ids = test_df_raw['Hospital_Id'].values\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': submission_ids,\n",
    "    'Transport_Cost': test_pred\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_knn_pca.csv', index=False)\n",
    "print(\"\\nSaved submission_knn_pca.csv\")\n",
    "print(submission_df.head())\n",
    "\n",
    "#explaination of variance by PCA\n",
    "try:\n",
    "    pca_obj = best_model.named_steps['pca']\n",
    "    explained = np.cumsum(pca_obj.explained_variance_ratio_)*100\n",
    "    print(\"\\nPCA cumulative explained variance (%):\", explained[:min(len(explained), 10)])\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# === End of script ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad06d67-6f75-4e0e-a617-124ace6afa87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the full ML pipeline (Strategy: KNN ONLY)...\n",
      "Trying alternative path...\n",
      "Original train data shape: (5000, 20)\n",
      "Original test data shape: (500, 19)\n",
      "Found 493 rows with non-positive cost. Setting them to 0.\n",
      "Combined data shape for preprocessing: (5500, 20)\n",
      "Starting feature engineering...\n",
      "Created 'Is_Missing' flags for missing columns.\n",
      "Feature engineering complete.\n",
      "Building preprocessing pipeline...\n",
      "Final shapes: X=(5000, 25), y=(5000,), X_test=(500, 25)\n",
      "\n",
      "--- Starting Model Tuning for KNN ---\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best parameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 20, 'leaf_size': 20}\n",
      "\n",
      "KNN RMSE (on validation): 44278.05\n",
      "KNN MSE (on validation): 1960545274.93\n",
      "Retraining KNN on full training data...\n",
      "\n",
      "--- DONE ---\n",
      "Submission file 'submission_KNN_ONLY_model.csv' created successfully.\n",
      "            Hospital_Id  Transport_Cost\n",
      "0          fffe33003400      263.338849\n",
      "1  fffe3700330036003600      182.750656\n",
      "2  fffe3300390038003400     1405.885967\n",
      "3      fffe310030003900      132.463422\n",
      "4  fffe3700330031003200      589.708534\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#Load Data\n",
    "try:\n",
    "    train_df_raw = pd.read_csv(\"medical_cost_prediction/train.csv\")\n",
    "    test_df_raw = pd.read_csv(\"medical_cost_prediction/test.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Trying alternative path...\")\n",
    "    try:\n",
    "        train_df_raw = pd.read_csv(\"trainvad.csv\")\n",
    "        test_df_raw = pd.read_csv(\"test.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not find train.csv or test.csv. {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"Original train data shape: {train_df_raw.shape}\")\n",
    "print(f\"Original test data shape: {test_df_raw.shape}\")\n",
    "\n",
    "test_hospital_ids = test_df_raw['Hospital_Id']\n",
    "\n",
    "# --- 3. Clean Target Variable & Log Transform ---\n",
    "target = train_df_raw['Transport_Cost'].copy()\n",
    "invalid_cost_indices = target[target <= 0].index\n",
    "print(f\"Found {len(invalid_cost_indices)} rows with non-positive cost. Setting them to 0.\")\n",
    "target.loc[invalid_cost_indices] = 0\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "train_df_processed = train_df_raw.drop('Transport_Cost', axis=1)\n",
    "train_df_processed['source'] = 'train'\n",
    "test_df_raw['source'] = 'test'\n",
    "df = pd.concat([train_df_processed, test_df_raw], ignore_index=True)\n",
    "print(f\"Combined data shape for preprocessing: {df.shape}\")\n",
    "\n",
    "#Feature Engineering\n",
    "missing_cols = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    'Equipment_Weight',\n",
    "    'Equipment_Type',\n",
    "    'Transport_Method',\n",
    "    'Rural_Hospital'\n",
    "]\n",
    "\n",
    "def preprocess_features(df_to_process):\n",
    "    print(\"Starting feature engineering...\")\n",
    "    df_processed = df_to_process.copy()\n",
    "    df_processed = df_processed.drop(['Supplier_Name'], axis=1)\n",
    "\n",
    "    # Missing value indicators\n",
    "    for col in missing_cols:\n",
    "        df_processed[col + '_Is_Missing'] = df_processed[col].isnull().astype(int)\n",
    "    print(\"Created 'Is_Missing' flags for missing columns.\")\n",
    "\n",
    "    # Date features\n",
    "    df_processed['Order_Placed_Date'] = pd.to_datetime(df_processed['Order_Placed_Date'])\n",
    "    df_processed['Delivery_Date'] = pd.to_datetime(df_processed['Delivery_Date'])\n",
    "    df_processed['Delivery_Time_Days'] = (df_processed['Delivery_Date'] - df_processed['Order_Placed_Date']).dt.days.clip(lower=0)\n",
    "    df_processed = df_processed.drop(['Order_Placed_Date', 'Delivery_Date'], axis=1)\n",
    "\n",
    "    # Location features\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_Location'].str.split(',').str[1].str.strip().str.split(' ').str[0]\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_State'].fillna('Unknown')\n",
    "    df_processed = df_processed.drop('Hospital_Location', axis=1)\n",
    "\n",
    "    # Binary features\n",
    "    binary_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', \n",
    "                   'Fragile_Equipment', 'Rural_Hospital', 'Hospital_Info']\n",
    "    for col in binary_cols:\n",
    "        df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)\n",
    "\n",
    "    # Fill numeric\n",
    "    df_processed['Equipment_Height'] = df_processed['Equipment_Height'].fillna(1)\n",
    "    df_processed['Equipment_Width'] = df_processed['Equipment_Width'].fillna(1)\n",
    "    df_processed['Equipment_Weight'] = df_processed['Equipment_Weight'].fillna(0)\n",
    "    df_processed['Equipment_Value'] = df_processed['Equipment_Value'].fillna(0)\n",
    "\n",
    "    # Derived/interaction features\n",
    "    df_processed['Equipment_Area'] = df_processed['Equipment_Height'] * df_processed['Equipment_Width']\n",
    "    df_processed['Value_Density'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Weight'] + 1e-6)\n",
    "    df_processed['Value_per_Height'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Height'] + 1e-6)\n",
    "    df_processed['Value_per_Width'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Width'] + 1e-6)\n",
    "    df_processed['Weight_per_Area'] = df_processed['Equipment_Weight'] / (df_processed['Equipment_Area'] + 1e-6)\n",
    "    df_processed['Cost_per_Day'] = df_processed['Base_Transport_Fee'] / (df_processed['Delivery_Time_Days'] + 1)\n",
    "    df_processed['Value_per_Area'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Area'] + 1e-6)\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df_processed\n",
    "\n",
    "df_featured = preprocess_features(df)\n",
    "\n",
    "#Preprocessing Pipeline\n",
    "print(\"Building preprocessing pipeline...\")\n",
    "\n",
    "numeric_features = ['Supplier_Reliability', \"Cost_per_Day\"]\n",
    "skewed_features = [\n",
    "    'Equipment_Value', 'Base_Transport_Fee', 'Value_Density', 'Equipment_Width', 'Equipment_Height',\n",
    "    'Equipment_Area', \"Value_per_Area\", \"Value_per_Height\", \"Value_per_Width\", \"Weight_per_Area\"\n",
    "]\n",
    "categorical_features = ['Equipment_Type', 'Transport_Method']\n",
    "binary_features = ['Fragile_Equipment', 'Rural_Hospital', 'Hospital_Info']\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "skewed_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('skew', skewed_pipeline, skewed_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "#Apply Preprocessing\n",
    "df_to_transform = df_featured.drop(['Hospital_Id', 'source'], axis=1)\n",
    "train_mask = df_featured['source'] == 'train'\n",
    "preprocessor.fit(df_to_transform[train_mask])\n",
    "df_final = preprocessor.transform(df_to_transform)\n",
    "\n",
    "train_mask_numpy = train_mask.values\n",
    "X = df_final[train_mask_numpy]\n",
    "X_test = df_final[~train_mask_numpy]\n",
    "y = target_log.reset_index(drop=True)\n",
    "\n",
    "print(f\"Final shapes: X={X.shape}, y={y.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#KNN Model\n",
    "print(\"\\n--- Starting Model Tuning for KNN ---\")\n",
    "\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "    'leaf_size': [20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=knn,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_knn = search.best_estimator_\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "#Evaluate on Validation Set\n",
    "y_pred_log = best_knn.predict(X_val)\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "mse = mean_squared_error(y_val_actual, y_pred_actual)\n",
    "print(f\"\\nKNN RMSE (on validation): {rmse:.2f}\")\n",
    "print(f\"KNN MSE (on validation): {mse:.2f}\")\n",
    "\n",
    "#Retrain on Full Data\n",
    "print(\"Retraining KNN on full training data...\")\n",
    "best_knn.fit(X, y)\n",
    "\n",
    "#Predict on Test Data\n",
    "test_pred_log = best_knn.predict(X_test)\n",
    "test_pred_actual = np.expm1(test_pred_log)\n",
    "test_pred_actual[test_pred_actual < 0] = 0  # Safety clip\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': test_hospital_ids,\n",
    "    'Transport_Cost': test_pred_actual\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_KNN.csv', index=False)\n",
    "\n",
    "print(\"\\n--- DONE ---\")\n",
    "print(\"Submission file 'submission_KNN.csv' created successfully.\")\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb4665-b61c-471c-9452-f406e085f93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying alternative path...\n",
      "Original train data shape: (5000, 20)\n",
      "Original test data shape: (500, 19)\n",
      "Found 493 rows with non-positive cost. Setting them to 0.\n",
      "Combined data shape for preprocessing: (5500, 20)\n",
      "Starting feature engineering...\n",
      "Created 'Is_Missing' flags for missing columns.\n",
      "Feature engineering complete.\n",
      "Building preprocessing + PCA pipeline...\n",
      "\n",
      "--- Starting Model Tuning for KNN + PCA ---\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#Load Data\n",
    "try:\n",
    "    train_df_raw = pd.read_csv(\"medical_cost_prediction/train.csv\")\n",
    "    test_df_raw = pd.read_csv(\"medical_cost_prediction/test.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Trying alternative path...\")\n",
    "    try:\n",
    "        train_df_raw = pd.read_csv(\"trainvad.csv\")\n",
    "        test_df_raw = pd.read_csv(\"test.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not find train.csv or test.csv. {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"Original train data shape: {train_df_raw.shape}\")\n",
    "print(f\"Original test data shape: {test_df_raw.shape}\")\n",
    "\n",
    "test_hospital_ids = test_df_raw['Hospital_Id']\n",
    "\n",
    "#Clean Target Variable and Log Transform\n",
    "target = train_df_raw['Transport_Cost'].copy()\n",
    "invalid_cost_indices = target[target <= 0].index\n",
    "print(f\"Found {len(invalid_cost_indices)} rows with non-positive cost. Setting them to 0.\")\n",
    "target.loc[invalid_cost_indices] = 0\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "train_df_processed = train_df_raw.drop('Transport_Cost', axis=1)\n",
    "train_df_processed['source'] = 'train'\n",
    "test_df_raw['source'] = 'test'\n",
    "df = pd.concat([train_df_processed, test_df_raw], ignore_index=True)\n",
    "print(f\"Combined data shape for preprocessing: {df.shape}\")\n",
    "\n",
    "#Feature Engineering\n",
    "missing_cols = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    'Equipment_Weight',\n",
    "    'Equipment_Type',\n",
    "    'Transport_Method',\n",
    "    'Rural_Hospital'\n",
    "]\n",
    "\n",
    "def preprocess_features(df_to_process):\n",
    "    print(\"Starting feature engineering...\")\n",
    "    df_processed = df_to_process.copy()\n",
    "    df_processed = df_processed.drop(['Supplier_Name'], axis=1)\n",
    "\n",
    "    for col in missing_cols:\n",
    "        df_processed[col + '_Is_Missing'] = df_processed[col].isnull().astype(int)\n",
    "    print(\"Created 'Is_Missing' flags for missing columns.\")\n",
    "\n",
    "    df_processed['Order_Placed_Date'] = pd.to_datetime(df_processed['Order_Placed_Date'])\n",
    "    df_processed['Delivery_Date'] = pd.to_datetime(df_processed['Delivery_Date'])\n",
    "    df_processed['Delivery_Time_Days'] = (df_processed['Delivery_Date'] - df_processed['Order_Placed_Date']).dt.days.clip(lower=0)\n",
    "    df_processed = df_processed.drop(['Order_Placed_Date', 'Delivery_Date'], axis=1)\n",
    "\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_Location'].str.split(',').str[1].str.strip().str.split(' ').str[0]\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_State'].fillna('Unknown')\n",
    "    df_processed = df_processed.drop('Hospital_Location', axis=1)\n",
    "\n",
    "    binary_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', \n",
    "                   'Fragile_Equipment', 'Rural_Hospital','Hospital_Info']\n",
    "    for col in binary_cols:\n",
    "        df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)\n",
    "\n",
    "    df_processed['Equipment_Height'] = df_processed['Equipment_Height'].fillna(1)\n",
    "    df_processed['Equipment_Width'] = df_processed['Equipment_Width'].fillna(1)\n",
    "    df_processed['Equipment_Weight'] = df_processed['Equipment_Weight'].fillna(0)\n",
    "    df_processed['Equipment_Value'] = df_processed['Equipment_Value'].fillna(0)\n",
    "\n",
    "    df_processed['Equipment_Area'] = df_processed['Equipment_Height'] * df_processed['Equipment_Width']\n",
    "    df_processed['Value_Density'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Weight'] + 1e-6)\n",
    "    df_processed['Value_per_Height'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Height'] + 1e-6)\n",
    "    df_processed['Value_per_Width'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Width'] + 1e-6)\n",
    "    df_processed['Weight_per_Area'] = df_processed['Equipment_Weight'] / (df_processed['Equipment_Area'] + 1e-6)\n",
    "    df_processed['Cost_per_Day'] = df_processed['Base_Transport_Fee'] / (df_processed['Delivery_Time_Days'] + 1)\n",
    "    df_processed['Value_per_Area'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Area'] + 1e-6)\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df_processed\n",
    "\n",
    "df_featured = preprocess_features(df)\n",
    "\n",
    "#Preprocessing Pipeline\n",
    "print(\"Building preprocessing + PCA pipeline...\")\n",
    "\n",
    "numeric_features = ['Supplier_Reliability', \"Cost_per_Day\"]\n",
    "skewed_features = [\n",
    "    'Equipment_Value', 'Base_Transport_Fee', 'Value_Density', 'Equipment_Width', 'Equipment_Height',\n",
    "    'Equipment_Area', \"Value_per_Area\", \"Value_per_Height\", \"Value_per_Width\", \"Weight_per_Area\"\n",
    "]\n",
    "categorical_features = ['Equipment_Type', 'Transport_Method']\n",
    "binary_features = ['Fragile_Equipment', 'Rural_Hospital', 'Hospital_Info']\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "skewed_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('skew', skewed_pipeline, skewed_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "#PCA + Model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('pca', PCA(n_components=0.95, random_state=42)),  # Retain 95% variance\n",
    "    ('knn', KNeighborsRegressor())\n",
    "])\n",
    "\n",
    "# --- 7. Prepare Data ---\n",
    "df_to_transform = df_featured.drop(['Hospital_Id', 'source'], axis=1)\n",
    "train_mask = df_featured['source'] == 'train'\n",
    "X = df_to_transform[train_mask]\n",
    "X_test = df_to_transform[~train_mask]\n",
    "y = target_log.reset_index(drop=True)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Model Tuning\n",
    "print(\"\\n--- Starting Model Tuning for KNN + PCA ---\")\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [0.90, 0.95, 0.98],\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__p': [1, 2]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "#Evaluate on Validation Set\n",
    "y_pred_log = best_model.predict(X_val)\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "print(f\"\\nKNN + PCA RMSE (on validation): {rmse:.2f}\")\n",
    "\n",
    "#Retrain on Full Data\n",
    "print(\"Retraining best KNN + PCA model on full training data...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "#Predict on Test Data\n",
    "test_pred_log = best_model.predict(X_test)\n",
    "test_pred_actual = np.expm1(test_pred_log)\n",
    "test_pred_actual[test_pred_actual < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': test_hospital_ids,\n",
    "    'Transport_Cost': test_pred_actual\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_KNN_PCA_model.csv', index=False)\n",
    "\n",
    "print(\"\\n--- DONE ---\")\n",
    "print(\"Submission file 'submission_KNN_PCA_model.csv' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc21e57d-72b1-4a3f-9980-0320afcb8ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
