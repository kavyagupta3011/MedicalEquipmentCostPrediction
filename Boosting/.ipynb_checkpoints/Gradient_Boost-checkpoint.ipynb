{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7bef0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting the full ML pipeline (Strategy: LightGBM ONLY)...\")\n",
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#Load Data\n",
    "try:\n",
    "    train_df_raw = pd.read_csv(\"train.csv\")\n",
    "    test_df_raw = pd.read_csv(\"test.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Trying alternative path\")\n",
    "    try:\n",
    "        train_df_raw = pd.read_csv(\"train.csv\")\n",
    "        test_df_raw = pd.read_csv(\"test.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL ERROR: Could not find train.csv or test.csv. {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"Original train data shape: {train_df_raw.shape}\")\n",
    "print(f\"Original test data shape: {test_df_raw.shape}\")\n",
    "\n",
    "test_hospital_ids = test_df_raw['Hospital_Id']\n",
    "\n",
    "#Clean Target Variable & Log Transform\n",
    "target = train_df_raw['Transport_Cost'].copy()\n",
    "invalid_cost_indices = target[target <= 0].index\n",
    "print(f\"Found {len(invalid_cost_indices)} rows with non-positive cost. Setting them to 0.\")\n",
    "target.loc[invalid_cost_indices] = 0\n",
    "target_log = np.log1p(target)\n",
    "\n",
    "train_df_processed = train_df_raw.drop('Transport_Cost', axis=1)\n",
    "train_df_processed['source'] = 'train'\n",
    "test_df_raw['source'] = 'test'\n",
    "df = pd.concat([train_df_processed, test_df_raw], ignore_index=True)\n",
    "print(f\"Combined data shape for preprocessing: {df.shape}\")\n",
    "\n",
    "#Feature Engineering\n",
    " # 2. We find all columns that have any missing data\n",
    "missing_cols = [\n",
    "    'Supplier_Reliability',\n",
    "    'Equipment_Height',\n",
    "    'Equipment_Width',\n",
    "    'Equipment_Weight',\n",
    "    'Equipment_Type',\n",
    "    'Transport_Method',\n",
    "    'Rural_Hospital'\n",
    "]\n",
    "def preprocess_features(df_to_process):\n",
    "    print(\"Starting feature engineering...\")\n",
    "    df_processed = df_to_process.copy()\n",
    "    df_processed = df_processed.drop(['Supplier_Name'], axis=1)\n",
    "\n",
    "   \n",
    "\n",
    "    # Create binary indicators (1 = missing, 0 = not missing)\n",
    "    for col in missing_cols:\n",
    "        df_processed[col + '_Is_Missing'] = df_processed[col].isnull().astype(int)\n",
    "    print(f\"Created 'Is_Missing' flags for these columns.\")\n",
    "    \n",
    "    # Date Features\n",
    "    df_processed['Order_Placed_Date'] = pd.to_datetime(df_processed['Order_Placed_Date'])\n",
    "    df_processed['Delivery_Date'] = pd.to_datetime(df_processed['Delivery_Date'])\n",
    "    df_processed['Delivery_Time_Days'] = (df_processed['Delivery_Date'] - df_processed['Order_Placed_Date']).dt.days.clip(lower=0)\n",
    "    #df_processed['Order_Year'] = df_processed['Order_Placed_Date'].dt.year\n",
    "    #df_processed['Order_Month'] = df_processed['Order_Placed_Date'].dt.month\n",
    "    #df_processed['Order_DayOfWeek'] = df_processed['Order_Placed_Date'].dt.dayofweek\n",
    "    df_processed = df_processed.drop(['Order_Placed_Date', 'Delivery_Date'], axis=1)\n",
    "\n",
    "    # Location Features\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_Location'].str.split(',').str[1].str.strip().str.split(' ').str[0]\n",
    "    df_processed['Hospital_State'] = df_processed['Hospital_State'].fillna('Unknown')\n",
    "    df_processed = df_processed.drop('Hospital_Location', axis=1)\n",
    "\n",
    "    # Binary Features\n",
    "    binary_cols = ['CrossBorder_Shipping', 'Urgent_Shipping', 'Installation_Service', \n",
    "                   'Fragile_Equipment', 'Rural_Hospital','Hospital_Info']\n",
    "    for col in binary_cols:\n",
    "        df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)\n",
    "\n",
    "    #Interaction Features\n",
    "    df_processed['Equipment_Height'] = df_processed['Equipment_Height'].fillna(1)\n",
    "    df_processed['Equipment_Width'] = df_processed['Equipment_Width'].fillna(1)\n",
    "    df_processed['Equipment_Weight'] = df_processed['Equipment_Weight'].fillna(0)\n",
    "    df_processed['Equipment_Value'] = df_processed['Equipment_Value'].fillna(0)\n",
    "\n",
    "    df_processed['Equipment_Area'] = df_processed['Equipment_Height'] * df_processed['Equipment_Width']\n",
    "    df_processed['Value_Density'] = df_processed['Equipment_Value'] / (df_processed['Equipment_Weight'] + 1e-6)\n",
    "    df_processed[\"Value_per_Height\"] = df_processed[\"Equipment_Value\"] / (df_processed[\"Equipment_Height\"] + 1e-6)\n",
    "    df_processed[\"Value_per_Width\"] = df_processed[\"Equipment_Value\"] / (df_processed[\"Equipment_Width\"] + 1e-6)\n",
    "    df_processed[\"Weight_per_Area\"] = df_processed[\"Equipment_Weight\"] / (df_processed[\"Equipment_Area\"] + 1e-6)\n",
    "    df_processed[\"Cost_per_Day\"] = df_processed[\"Base_Transport_Fee\"] / (df_processed[\"Delivery_Time_Days\"] + 1)\n",
    "    df_processed[\"Value_per_Area\"] = df_processed[\"Equipment_Value\"] / (df_processed[\"Equipment_Area\"] + 1e-6)\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n",
    "    return df_processed\n",
    "\n",
    "df_featured = preprocess_features(df)\n",
    "missing_flag_features = [col + '_Is_Missing' for col in missing_cols]\n",
    "#Preprocessing Pipeline\n",
    "print(\"Building preprocessing pipeline...\")\n",
    "\n",
    "numeric_features = ['Supplier_Reliability',\"Cost_per_Day\"]\n",
    "skewed_features = [ 'Equipment_Value', 'Base_Transport_Fee', 'Value_Density','Equipment_Width','Equipment_Height',\n",
    "                   'Equipment_Area',\"Value_per_Area\",\"Value_per_Height\",\"Value_per_Width\",\"Weight_per_Area\"\n",
    "                  ]\n",
    "categorical_features = ['Equipment_Type', 'Transport_Method']\n",
    "binary_features = [\n",
    "                   'Fragile_Equipment', 'Rural_Hospital','Hospital_Info']\n",
    "binary_features=binary_features \n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    #('scaler',  RobustScaler())\n",
    "])\n",
    "\n",
    "skewed_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('log_transform', FunctionTransformer(np.log1p, validate=False))\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_pipeline, numeric_features),\n",
    "        ('skew', skewed_pipeline, skewed_features),\n",
    "        ('cat', categorical_pipeline, categorical_features),\n",
    "        ('binary', 'passthrough', binary_features)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "#Apply Preprocessing\n",
    "df_to_transform = df_featured.drop(['Hospital_Id', 'source'], axis=1)\n",
    "train_mask = df_featured['source'] == 'train'\n",
    "preprocessor.fit(df_to_transform[train_mask])\n",
    "df_final = preprocessor.transform(df_to_transform)\n",
    "\n",
    "train_mask_numpy = train_mask.values\n",
    "X = df_final[train_mask_numpy]\n",
    "X_test = df_final[~train_mask_numpy]\n",
    "y = target_log.reset_index(drop=True)\n",
    "\n",
    "print(f\"Final shapes: X={X.shape}, y={y.shape}, X_test={X_test.shape}\")\n",
    "# Suppose y is your target (Log_Transport_Cost)\n",
    "#num_bins = 10  # 10 bins based on quantiles\n",
    "#y_binned = pd.qcut(y, q=num_bins, labels=False, duplicates='drop')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#LightGBM Model\n",
    "print(\"\\n--- Starting Model Tuning for LightGBM ---\")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [200, 400, 600, 800],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.3, 0.5, 1],\n",
    "    'reg_lambda': [0, 0.5, 1, 2, 5],\n",
    "    'min_split_gain': [0, 0.01, 0.05],\n",
    "    'num_leaves': [31, 63, 127],\n",
    "    'max_depth': [5, 7, 9]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "\n",
    "\n",
    "#Evaluate on Validation Set\n",
    "y_pred_log = best_model.predict(X_val)\n",
    "y_pred_actual = np.expm1(y_pred_log)\n",
    "y_val_actual = np.expm1(y_val)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_actual, y_pred_actual))\n",
    "print(f\"\\nLightGBM RMSE (on validation): {rmse:.2f}\")\n",
    "mse=mean_squared_error(y_val_actual, y_pred_actual)\n",
    "print(f\"\\nLightGBM MSE (on validation): {mse:.2f}\")\n",
    "lgb.plot_importance(best_model, max_num_features=20, importance_type='gain')\n",
    "\n",
    "#Retrain on Full Data\n",
    "print(\"Retraining LightGBM on full training data...\")\n",
    "best_model.fit(X, y)\n",
    "\n",
    "#Predict on Test Data\n",
    "test_pred_log = best_model.predict(X_test)\n",
    "test_pred_actual = np.expm1(test_pred_log)\n",
    "test_pred_actual[test_pred_actual < 0] = 0  # Safety clip\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'Hospital_Id': test_hospital_ids,\n",
    "    'Transport_Cost': test_pred_actual\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission_LIGHTGBM_ONLY_model.csv', index=False)\n",
    "\n",
    "print(\"\\n--- DONE ---\")\n",
    "print(\"Submission file 'submission_LIGHTGBM_ONLY_model.csv' created successfully.\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
