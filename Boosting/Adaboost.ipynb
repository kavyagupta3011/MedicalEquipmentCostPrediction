{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1842a4ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading file: [Errno 2] No such file or directory: 'sample_submission.csv'\n",
      "Data loaded successfully.\n",
      "Index set to Hospital_Id.\n",
      "Target variable 'y' prepared (clipped at 0 + log1p transform).\n",
      "Combined DataFrame shape: (5500, 19)\n",
      "Date features engineered (Delivery_Time_Days clipped at 0).\n",
      "Numerical missing values imputed.\n",
      "Categorical missing values imputed.\n",
      "Binary 'Yes'/'No' columns encoded.\n",
      "Dropped 'Supplier_Name' column.\n",
      "One-Hot Encoding complete.\n",
      "Column alignment complete.\n",
      "Columns to scale: ['Supplier_Reliability', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight', 'Equipment_Value', 'Base_Transport_Fee', 'Delivery_Time_Days', 'Delivery_Time_Days_Corrected']\n",
      "Scaling complete.\n",
      "X (full train): (5000, 23)\n",
      "y (full train): (5000,)\n",
      "X_train (split): (4000, 23)\n",
      "y_train (split): (4000,)\n",
      "X_val (split): (1000, 23)\n",
      "y_val (split): (1000,)\n",
      "X_test (full test): (500, 23)\n",
      "--- All variables are now defined. ---\n",
      "\n",
      "AdaBoost Regressor\n",
      "Training basic AdaBoost model\n",
      "--- Performance for AdaBoost (Basic) ---\n",
      "RMSE: 2.2214\n",
      "R2 Score: 0.1288\n",
      "---------------------------------\n",
      "\n",
      "Starting AdaBoost Hyperparameter Tuning\n",
      "Running GridSearchCV for AdaBoost... (This will be slower but should not crash)\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=200; total time=   1.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=200; total time=   1.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.01, n_estimators=200; total time=   1.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=100; total time=   1.0s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=200; total time=   1.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=200; total time=   1.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.05, n_estimators=200; total time=   2.0s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=50; total time=   0.4s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=100; total time=   0.9s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=100; total time=   0.8s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=100; total time=   0.8s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=200; total time=   1.7s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=200; total time=   1.8s\n",
      "[CV] END estimator__max_depth=3, learning_rate=0.1, n_estimators=200; total time=   1.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=100; total time=   1.4s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=100; total time=   1.4s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=100; total time=   1.4s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=200; total time=   2.7s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=200; total time=   2.8s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.01, n_estimators=200; total time=   2.7s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=100; total time=   1.3s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=100; total time=   1.3s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=100; total time=   1.3s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=200; total time=   2.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=200; total time=   2.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.05, n_estimators=200; total time=   2.5s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=50; total time=   0.6s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=100; total time=   1.2s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=100; total time=   1.2s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=100; total time=   1.2s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=200; total time=   2.7s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=200; total time=   2.4s\n",
      "[CV] END estimator__max_depth=5, learning_rate=0.1, n_estimators=200; total time=   2.4s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=100; total time=   1.7s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=100; total time=   1.7s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=100; total time=   1.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=200; total time=   3.7s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=200; total time=   3.7s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.01, n_estimators=200; total time=   3.7s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=50; total time=   0.9s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=100; total time=   1.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=100; total time=   1.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=100; total time=   1.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=200; total time=   3.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=200; total time=   4.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.05, n_estimators=200; total time=   3.5s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=50; total time=   0.9s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=50; total time=   0.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=100; total time=   1.8s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=100; total time=   1.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=100; total time=   1.6s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=200; total time=   3.2s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=200; total time=   3.4s\n",
      "[CV] END estimator__max_depth=7, learning_rate=0.1, n_estimators=200; total time=   3.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=50; total time=   1.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=100; total time=   2.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=100; total time=   2.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=100; total time=   2.6s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=200; total time=   4.9s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=200; total time=   5.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.01, n_estimators=200; total time=   4.8s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=50; total time=   1.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=100; total time=   2.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=100; total time=   2.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=100; total time=   2.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=200; total time=   4.5s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=200; total time=   4.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.05, n_estimators=200; total time=   4.7s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=50; total time=   1.1s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=50; total time=   1.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=100; total time=   2.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=100; total time=   2.4s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=100; total time=   2.2s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=200; total time=   4.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=200; total time=   4.3s\n",
      "[CV] END estimator__max_depth=10, learning_rate=0.1, n_estimators=200; total time=   4.4s\n",
      "\n",
      "Best parameters found for AdaBoost: {'estimator__max_depth': 5, 'learning_rate': 0.01, 'n_estimators': 50}\n",
      "--- Performance for AdaBoost (Tuned) ---\n",
      "RMSE: 2.0029\n",
      "R2 Score: 0.2917\n",
      "---------------------------------\n",
      "--- Performance for AdaBoost (Actual) ---\n",
      "RMSE: 37188.6154\n",
      "R2 Score: 0.3592\n",
      "---------------------------------\n",
      "\n",
      "AdaBoost Feature Importance \n",
      "Saved 'adaboost_features_clipped.png'\n",
      "\n",
      "Training final AdaBoost model on ALL training data...\n",
      "Making predictions on test.csv...\n",
      "\n",
      "'submission_adaboost_clipped.csv' created successfully!\n",
      "            Hospital_Id  Transport_Cost\n",
      "0          fffe33003400      260.618805\n",
      "1  fffe3700330036003600      155.766067\n",
      "2  fffe3300390038003400     1003.189931\n",
      "3      fffe310030003900      114.950034\n",
      "4  fffe3700330031003200      365.187986\n",
      "File shape: (500, 2) (Should be 500, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#helper function\n",
    "#Calculates and prints the RMSE and R2 score for a model's predictions.\n",
    "def print_model_performance(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"--- Performance for {model_name} ---\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "#load data\n",
    "try:\n",
    "    #Attempt to read the CSV files into pandas DataFrames\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    sample_submission_df = pd.read_csv(\"sample_submission.csv\")\n",
    "except FileNotFoundError as e:\n",
    "    #Handle the case where a file is not found\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit() #Exit the script if data can't be loaded\n",
    "except Exception as e:\n",
    "    #Handle other potential errors during file loading\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    exit() #Exit the script\n",
    "print(\"Data loaded successfully.\")\n",
    "\n",
    "#set index\n",
    "try:\n",
    "    #Set the 'Hospital_Id' column as the index for both train and test DataFrames\n",
    "    #This is useful for aligning data and submissions\n",
    "    train_df = train_df.set_index('Hospital_Id')\n",
    "    test_df = test_df.set_index('Hospital_Id')\n",
    "    print(\"Index set to Hospital_Id.\")\n",
    "except KeyError:\n",
    "    #Handle the case where 'Hospital_Id' column doesn't exist\n",
    "    print(\"Hospital_Id column not found. Please check your CSV files.\")\n",
    "    exit()\n",
    "\n",
    "#target variable preparation\n",
    "#clip(lower=0) to ensure there are no negatives cost values\n",
    "y_original = train_df['Transport_Cost'].clip(lower=0)\n",
    "\n",
    "#apply log transform\n",
    "#Apply a log1p transformation (log(1 + x)) to the target variable\n",
    "#This helps normalize the distribution of the target, which is often skewed\n",
    "#It's particularly useful for models sensitive to outliers\n",
    "y_log = np.log1p(y_original)\n",
    "\n",
    "#Drop the original target variable from the training DataFrame\n",
    "train_df = train_df.drop('Transport_Cost', axis=1)\n",
    "print(\"Target variable 'y' prepared (clipped at 0 + log1p transform).\")\n",
    "\n",
    "#combine data\n",
    "#Add a 'source' column to distinguish between train and test data\n",
    "train_df['source'] = 'train'\n",
    "test_df['source'] = 'test'\n",
    "#Concatenate the train and test DataFrames into a single DataFrame\n",
    "#This simplifies preprocessing, as all transformations can be applied at once\n",
    "combined_df = pd.concat([train_df, test_df], axis=0)\n",
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "\n",
    "#feature engineering(dates)\n",
    "try:\n",
    "    combined_df['Order_Placed_Date'] = pd.to_datetime(combined_df['Order_Placed_Date'], format='%m/%d/%y')\n",
    "    combined_df['Delivery_Date'] = pd.to_datetime(combined_df['Delivery_Date'], format='%m/%d/%y')\n",
    "    \n",
    "    #calculate days, which might be negative\n",
    "    combined_df['Delivery_Time_Days'] = (combined_df['Delivery_Date'] - combined_df['Order_Placed_Date']).dt.days\n",
    "    #nOW, clip at 0\n",
    "    combined_df['Delivery_Time_Days_Corrected'] = (combined_df['Delivery_Time_Days'] <0).astype(int)\n",
    "    combined_df['Delivery_Time_Days'] = combined_df['Delivery_Time_Days'].clip(lower=0)\n",
    "    \n",
    "    #continue with other date features\n",
    "    combined_df['Order_Year'] = combined_df['Order_Placed_Date'].dt.year\n",
    "    combined_df = combined_df.drop(['Order_Placed_Date', 'Delivery_Date'], axis=1)\n",
    "    print(\"Date features engineered (Delivery_Time_Days clipped at 0).\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during date feature engineering: {e}\")\n",
    "\n",
    "#feature engineering(location)\n",
    "combined_df = combined_df.drop('Hospital_Location', axis=1)\n",
    "#print(\"Location features engineered.\")\n",
    "\n",
    "#handle missing values\n",
    "#Define numerical columns to impute\n",
    "num_cols_impute = ['Supplier_Reliability', 'Equipment_Height', 'Equipment_Width', 'Equipment_Weight']\n",
    "for col in num_cols_impute:\n",
    "    median_val = combined_df[col].median()\n",
    "    combined_df[col] = combined_df[col].fillna(median_val)\n",
    "print(\"Numerical missing values imputed.\")\n",
    "\n",
    "#Define categorical columns to impute\n",
    "cat_cols_impute = ['Equipment_Type', 'Transport_Method']\n",
    "for col in cat_cols_impute:\n",
    "    if not combined_df[col].mode().empty:\n",
    "        mode_val = combined_df[col].mode()[0]\n",
    "        combined_df[col] = combined_df[col].fillna(mode_val)\n",
    "    else:\n",
    "        combined_df[col] = combined_df[col].fillna('Unknown')\n",
    "combined_df['Rural_Hospital'] = combined_df['Rural_Hospital'].fillna('No')\n",
    "print(\"Categorical missing values imputed.\")\n",
    "\n",
    "#categorical encoding\n",
    "combined_df = combined_df.drop('Urgent_Shipping', axis=1)\n",
    "combined_df = combined_df.drop('CrossBorder_Shipping', axis=1)\n",
    "combined_df = combined_df.drop('Installation_Service', axis=1)\n",
    "binary_cols = ['Fragile_Equipment', 'Rural_Hospital']\n",
    "for col in binary_cols:\n",
    "    combined_df[col] = combined_df[col].map({'Yes': 1, 'No': 0})\n",
    "print(\"Binary 'Yes'/'No' columns encoded.\")\n",
    "\n",
    "combined_df = combined_df.drop('Supplier_Name', axis=1)\n",
    "print(\"Dropped 'Supplier_Name' column.\")\n",
    "\n",
    "ohe_cols = ['Equipment_Type', 'Transport_Method', 'Hospital_Info']\n",
    "combined_df = pd.get_dummies(combined_df, columns=ohe_cols, drop_first=True, dummy_na=True)\n",
    "print(\"One-Hot Encoding complete.\")\n",
    "\n",
    "#split and scale\n",
    "#Split the combined DataFrame back into training and test sets using the 'source' column\n",
    "X = combined_df[combined_df['source'] == 'train'].drop('source', axis=1)\n",
    "X_test = combined_df[combined_df['source'] == 'test'].drop('source', axis=1)\n",
    "y = y_log\n",
    "\n",
    "train_cols = set(X.columns)\n",
    "test_cols = set(X_test.columns)\n",
    "\n",
    "missing_in_test = list(train_cols - test_cols)\n",
    "for col in missing_in_test:\n",
    "    X_test[col] = 0\n",
    "missing_in_train = list(test_cols - train_cols)\n",
    "for col in missing_in_train:\n",
    "    X[col] = 0\n",
    "\n",
    "X_test = X_test[X.columns]\n",
    "print(\"Column alignment complete.\")\n",
    "\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns\n",
    "ohe_generated_cols = [col for col in X.columns if col.startswith(tuple(ohe_cols))]\n",
    "binary_cols_to_exclude = set(binary_cols + ['Order_Year'])\n",
    "cols_to_exclude = set.union(binary_cols_to_exclude, set(ohe_generated_cols))\n",
    "cols_to_scale = [col for col in numerical_cols if col not in cols_to_exclude and col not in ohe_generated_cols]\n",
    "print(f\"Columns to scale: {cols_to_scale}\")\n",
    "\n",
    "if cols_to_scale:\n",
    "    scaler = StandardScaler()\n",
    "    X[cols_to_scale] = scaler.fit_transform(X[cols_to_scale])\n",
    "    X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "    print(\"Scaling complete.\")\n",
    "else:\n",
    "    print(\"No columns identified for scaling.\")\n",
    "\n",
    "#Create Train/Validation Split\n",
    "#Split the full training data (X, y) into a new training set and a validation set\n",
    "#'test_size=0.2' holds out 20% of the data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X (full train): {X.shape}\")\n",
    "print(f\"y (full train): {y.shape}\")\n",
    "print(f\"X_train (split): {X_train.shape}\")\n",
    "print(f\"y_train (split): {y_train.shape}\")\n",
    "print(f\"X_val (split): {X_val.shape}\")\n",
    "print(f\"y_val (split): {y_val.shape}\")\n",
    "print(f\"X_test (full test): {X_test.shape}\")\n",
    "print(\"--- All variables are now defined. ---\")\n",
    "\n",
    "\n",
    "#adaboost regressor\n",
    "\n",
    "print(\"\\nAdaBoost Regressor\")\n",
    "\n",
    "#basic model\n",
    "base_est = DecisionTreeRegressor(max_depth=5)\n",
    "ada_basic = AdaBoostRegressor(estimator=base_est, n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "print(\"Training basic AdaBoost model\")\n",
    "ada_basic.fit(X_train, y_train)\n",
    "\n",
    "preds_basic_ada = ada_basic.predict(X_val)\n",
    "print_model_performance(y_val, preds_basic_ada, \"AdaBoost (Basic)\")\n",
    "\n",
    "\n",
    "#Detailed Hyperparameter Tuning (GridSearchCV)\n",
    "print(\"\\nStarting AdaBoost Hyperparameter Tuning\")\n",
    "\n",
    "param_grid_ada = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'estimator__max_depth': [3, 5, 7, 10]\n",
    "}\n",
    "\n",
    "#Changed n_jobs=-1 to n_jobs=1 to prevent MemoryError.\n",
    "grid_search_ada = GridSearchCV(\n",
    "    estimator=AdaBoostRegressor(estimator=DecisionTreeRegressor(random_state=42), random_state=42),\n",
    "    param_grid=param_grid_ada,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=1  # <-- CHANGED FROM -1 to 1\n",
    ")\n",
    "\n",
    "print(\"Running GridSearchCV for AdaBoost... (This will be slower but should not crash)\")\n",
    "grid_search_ada.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters found for AdaBoost: {grid_search_ada.best_params_}\")\n",
    "\n",
    "#Check Tuned Model Performance\n",
    "best_ada_model = grid_search_ada.best_estimator_\n",
    "preds_tuned_ada = best_ada_model.predict(X_val)\n",
    "preds_actual= np.expm1(preds_tuned_ada)\n",
    "y_actuall = np.expm1(y_val)\n",
    "print_model_performance(y_val, preds_tuned_ada, \"AdaBoost (Tuned)\")\n",
    "print_model_performance(y_actuall, preds_actual, \"AdaBoost (Actual)\")\n",
    "\n",
    "#Feature Importance\n",
    "print(\"\\nAdaBoost Feature Importance \")\n",
    "importances_ada = best_ada_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feat_imp_ada = pd.Series(importances_ada, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "feat_imp_ada.head(20).plot(kind='barh')\n",
    "plt.title('Top 20 Features (AdaBoost)')\n",
    "plt.savefig('adaboost_features_clipped.png')\n",
    "print(\"Saved 'adaboost_features_clipped.png'\")\n",
    "plt.clf()\n",
    "\n",
    "#Final Submission (AdaBoost)\n",
    "print(\"\\nTraining final AdaBoost model on ALL training data...\")\n",
    "final_ada_model = grid_search_ada.best_estimator_\n",
    "final_ada_model.fit(X, y)\n",
    "\n",
    "print(\"Making predictions on test.csv...\")\n",
    "test_preds_log_ada = final_ada_model.predict(X_test)\n",
    "\n",
    "test_preds_ada = np.expm1(test_preds_log_ada)\n",
    "\n",
    "#Ensure no negative costs\n",
    "test_preds_ada[test_preds_ada < 0] = 0\n",
    "\n",
    "#Create submission file with a new name\n",
    "submission_ada = pd.DataFrame({\n",
    "    'Hospital_Id': X_test.index,\n",
    "    'Transport_Cost': test_preds_ada\n",
    "})\n",
    "\n",
    "submission_ada.to_csv('submission_adaboost_clipped.csv', index=False)\n",
    "print(\"\\n'submission_adaboost_clipped.csv' created successfully!\")\n",
    "print(submission_ada.head())\n",
    "print(f\"File shape: {submission_ada.shape} (Should be 500, 2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d3521-f2d3-4dc5-9ae5-ccbcd6eafbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
